{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_prices(close_prices, freq='M'):\n",
    "    \"\"\"\n",
    "    Resample close prices for each ticker at specified frequency.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close_prices : DataFrame\n",
    "        Close prices for each ticker and date\n",
    "    freq : str\n",
    "        What frequency to sample at\n",
    "        For valid freq choices, see http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    prices_resampled : DataFrame\n",
    "        Resampled prices for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Returning directly the data grouped by the frequency received and taking the last number that represents the\n",
    "    # closing data\n",
    "    return close_prices.resample(freq).last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_returns(prices):\n",
    "    \"\"\"\n",
    "    Compute log returns for each ticker.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_returns : DataFrame\n",
    "        Log returns for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Using numpy log function to calculate the log return\n",
    "    return np.log(prices)-np.log(prices.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_returns(returns, shift_n):\n",
    "    \"\"\"\n",
    "    Generate shifted returns\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    shift_n : int\n",
    "        Number of periods to move, can be positive or negative\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    shifted_returns : DataFrame\n",
    "        Shifted returns for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    # Returning directly the dataframe shifted the period 'shift_n'\n",
    "    return returns.shift(shift_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(prev_returns, top_n):\n",
    "    \"\"\"\n",
    "    Select the top performing stocks\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    prev_returns : DataFrame\n",
    "        Previous shifted returns for each ticker and date\n",
    "    top_n : int\n",
    "        The number of top performing stocks to get\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    top_stocks : DataFrame\n",
    "        Top stocks for each ticker and date marked with a 1\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # First we create the empty dataframe\n",
    "    top_stocks = pd.DataFrame(columns=prev_returns.columns)\n",
    "\n",
    "    # Now we iterate for every row\n",
    "    for index, row in prev_returns.iterrows():\n",
    "        #and append to the empty database every old row but with only the largest numbers as data\n",
    "        top_stocks.loc[index] = (row.nlargest(top_n))\n",
    "    # At returns, we apply a boolean check and convert the result to integers numbers (true=1, False=0) \n",
    "    return top_stocks.notna().astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_returns(df_long, df_short, lookahead_returns, n_stocks):\n",
    "    \"\"\"\n",
    "    Compute expected returns for the portfolio, assuming equal investment in each long/short stock.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_long : DataFrame\n",
    "        Top stocks for each ticker and date marked with a 1\n",
    "    df_short : DataFrame\n",
    "        Bottom stocks for each ticker and date marked with a 1\n",
    "    lookahead_returns : DataFrame\n",
    "        Lookahead returns for each ticker and date\n",
    "    n_stocks: int\n",
    "        The number number of stocks chosen for each month\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    portfolio_returns : DataFrame\n",
    "        Expected portfolio returns for each ticker and date\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # df_long and df_short are already multiplied by the n_stocks, so we susbtract both datas multiply them by our \n",
    "    # returns expected data (expected_portfolio_returns_by_date) and divide that for the n_stocks\n",
    "    \n",
    "    return (df_long - df_short) * lookahead_returns / n_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_alpha(expected_portfolio_returns_by_date):\n",
    "    \"\"\"\n",
    "    Perform a t-test with the null hypothesis being that the expected mean return is zero.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    expected_portfolio_returns_by_date : Pandas Series\n",
    "        Expected portfolio returns for each date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_value\n",
    "        T-statistic from t-test\n",
    "    p_value\n",
    "        Corresponding p-value\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Using stats.ttest_1samp to calculate de p_value. It returns two values.\n",
    "    # We stablished the series as the expected_portfolio_returns_by_date, and the null hypthesis as 0\n",
    "    \n",
    "    t_value, p_value = stats.ttest_1samp(expected_portfolio_returns_by_date, 0)\n",
    "\n",
    "    return t_value, p_value/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_high_lows_lookback(high, low, lookback_days):\n",
    "    \"\"\"\n",
    "    Get the highs and lows in a lookback window.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    high : DataFrame\n",
    "        High price for each ticker and date\n",
    "    low : DataFrame\n",
    "        Low price for each ticker and date\n",
    "    lookback_days : int\n",
    "        The number of days to look back\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookback_high : DataFrame\n",
    "        Lookback high price for each ticker and date\n",
    "    lookback_low : DataFrame\n",
    "        Lookback low price for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Just we implement the rolling function and the max() and min() value fo each\n",
    "    # sticker in the dataframe.\n",
    "    # To include only the working window before the current day, we just shift the\n",
    "    # data one day.\n",
    "    lookback_high = high.shift(1).rolling(window = lookback_days).max()\n",
    "    lookback_low = low.shift(1).rolling(window = lookback_days).min()\n",
    "\n",
    "    return lookback_high, lookback_low\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_long_short(close, lookback_high, lookback_low):\n",
    "    \"\"\"\n",
    "    Generate the signals long, short, and do nothing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookback_high : DataFrame\n",
    "        Lookback high price for each ticker and date\n",
    "    lookback_low : DataFrame\n",
    "        Lookback low price for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    long_short : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Just some conditions and math\n",
    "    long_short = -1 * (lookback_low > close) + 1 * (lookback_high < close)\n",
    "    \n",
    "    return long_short.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_signals(signals, window_size):\n",
    "    \"\"\"\n",
    "    Clear out signals in a Series of just long or short signals.\n",
    "    \n",
    "    Remove the number of signals down to 1 within the window size time period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signals : Pandas Series\n",
    "        The long, short, or do nothing signals\n",
    "    window_size : int\n",
    "        The number of days to have a single signal       \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    signals : Pandas Series\n",
    "        Signals with the signals removed from the window size\n",
    "    \"\"\"\n",
    "    # Start with buffer of window size\n",
    "    # This handles the edge case of calculating past_signal in the beginning\n",
    "    clean_signals = [0]*window_size\n",
    "    \n",
    "    for signal_i, current_signal in enumerate(signals):\n",
    "        # Check if there was a signal in the past window_size of days\n",
    "        has_past_signal = bool(sum(clean_signals[signal_i:signal_i+window_size]))\n",
    "        # Use the current signal if there's no past signal, else 0/False\n",
    "        clean_signals.append(not has_past_signal and current_signal)\n",
    "        \n",
    "    # Remove buffer\n",
    "    clean_signals = clean_signals[window_size:]\n",
    "\n",
    "    # Return the signals as a Series of Ints\n",
    "    return pd.Series(np.array(clean_signals).astype(np.int), signals.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_signals(signal, lookahead_days):\n",
    "    \"\"\"\n",
    "    Filter out signals in a DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    lookahead_days : int\n",
    "        The number of days to look ahead\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered_signal : DataFrame\n",
    "        The filtered long, short, and do nothing signals for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # First of all we will work with every column independently\n",
    "    filtered_signal = signal.copy()\n",
    "    for col in signal.columns:\n",
    "        # For each column we will send to the clear_signals function only one kind\n",
    "        # of signal, lows or high. We filter the column for lows first: checking\n",
    "        # the values below zero.\n",
    "        filtered_signal_low = clear_signals(signal[col]<0, lookahead_days) * -1\n",
    "        \n",
    "        # Then we make the same process for the high signals, or over zero\n",
    "        filtered_signal_high = clear_signals(signal[col]>0, lookahead_days) * 1\n",
    "        \n",
    "        # Finally we change the original column for sum of both results\n",
    "        filtered_signal[col] = filtered_signal_low + filtered_signal_high\n",
    "    \n",
    "    return filtered_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_prices(close, lookahead_days):\n",
    "    \"\"\"\n",
    "    Get the lookahead prices for `lookahead_days` number of days.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookahead_days : int\n",
    "        The number of days to look ahead\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookahead_prices : DataFrame\n",
    "        The lookahead prices for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Just shiftting the whole prices the same amount of days as lookahead_days\n",
    "    lookahead_prices = close.shift(-lookahead_days)\n",
    "    \n",
    "    return lookahead_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return_lookahead(close, lookahead_prices):\n",
    "    \"\"\"\n",
    "    Calculate the log returns from the lookahead days to the signal day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    lookahead_prices : DataFrame\n",
    "        The lookahead prices for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lookahead_returns : DataFrame\n",
    "        The lookahead log returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Just we calculate the log of lookahead_pruces and substract the \n",
    "    # log of the close prices to it.\n",
    "    lookahead_returns = np.log(lookahead_prices) - np.log(close)\n",
    "    \n",
    "    return lookahead_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_return(signal, lookahead_returns):\n",
    "    \"\"\"\n",
    "    Compute the signal returns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : DataFrame\n",
    "        The long, short, and do nothing signals for each ticker and date\n",
    "    lookahead_returns : DataFrame\n",
    "        The lookahead log returns for each ticker and date\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    signal_return : DataFrame\n",
    "        Signal returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # We just implement a multiplication of the signals by each return for those\n",
    "    # days ahead.\n",
    "    signal_return = signal * lookahead_returns\n",
    "    \n",
    "    \n",
    "    return signal_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest\n",
    "\n",
    "\n",
    "def calculate_kstest(long_short_signal_returns):\n",
    "    \"\"\"\n",
    "    Calculate the KS-Test against the signal returns with a long or short signal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    long_short_signal_returns : DataFrame\n",
    "        The signal returns which have a signal.\n",
    "        This DataFrame contains two columns, \"ticker\" and \"signal_return\"\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ks_values : Pandas Series\n",
    "        KS static for all the tickers\n",
    "    p_values : Pandas Series\n",
    "        P value for all the tickers\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # First we create a copy of the original returns data\n",
    "    lssr = long_short_signal_returns.copy()\n",
    "    \n",
    "    # To apply de Kolmogorov test we need to rescale the data.\n",
    "    # To rescale the data we substract the mean and divide every\n",
    "    # return by the global standard deviation.\n",
    "    lssr['signal_return'] = (lssr['signal_return']-\\\n",
    "                             lssr['signal_return'].mean())/\\\n",
    "                            np.std(lssr['signal_return'],ddof=0) \n",
    "    \n",
    "    # grouping the data per tickers.\n",
    "    lssr_group = lssr.groupby('ticker').signal_return\n",
    "    \n",
    "    # Initializing the list for ne\n",
    "    ks_values, p_values, indexes = [],[],[]\n",
    "    \n",
    "    for index, series in lssr_group:\n",
    "        ks, p = kstest(series, 'norm' )\n",
    "        indexes.append(index)\n",
    "        ks_values.append(ks)\n",
    "        p_values.append(p)\n",
    "    \n",
    "    return pd.Series(ks_values, index=indexes), pd.Series(p_values, index=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(ks_values, p_values, ks_threshold, pvalue_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Find outlying symbols using KS values and P-values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ks_values : Pandas Series\n",
    "        KS static for all the tickers\n",
    "    p_values : Pandas Series\n",
    "        P value for all the tickers\n",
    "    ks_threshold : float\n",
    "        The threshold for the KS statistic\n",
    "    pvalue_threshold : float\n",
    "        The threshold for the p-value\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    outliers : set of str\n",
    "        Symbols that are outliers\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Creating the initial set\n",
    "    outliers = set()\n",
    "    \n",
    "    # Now check for every ticker their ks and p values and compare them with\n",
    "    # the tresholds received. In case positive we add the ticker to our set.\n",
    "    for ticker in ks_values.index:\n",
    "        if (ks_values.loc[ticker] > ks_threshold) and (p_values.loc[ticker] < pvalue_threshold):\n",
    "            outliers.add(ticker)\n",
    "    \n",
    "    # Final step\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dollar_volume_weights(close, volume):\n",
    "    \"\"\"\n",
    "    Generate dollar volume weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    close : DataFrame\n",
    "        Close price for each ticker and date\n",
    "    volume : str\n",
    "        Volume for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dollar_volume_weights : DataFrame\n",
    "        The dollar volume weights for each ticker and date\n",
    "    \"\"\"\n",
    "    assert close.index.equals(volume.index)\n",
    "    assert close.columns.equals(volume.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # We directly return the resulting dataframe:\n",
    "    # We multiply both data frame to have the total amount traded each day,\n",
    "    # then divide all the dataframe for the sum of each row or\n",
    "    # the total amount traded for all the tickers the same day.\n",
    "    \n",
    "    clo_vol = close * volume\n",
    "\n",
    "    return clo_vol.div((clo_vol).sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dividend_weights(dividends):\n",
    "    \"\"\"\n",
    "    Calculate dividend weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dividends : DataFrame\n",
    "        Dividend for each stock and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dividend_weights : DataFrame\n",
    "        Weights for each stock and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # We directly return the resulting dataframe:\n",
    "    # We apply the cumulative sum of dividends for each ticker during time,\n",
    "    # then divide all this new cumulative dividends dataframe by\n",
    "    # the total amount of dividends each day\n",
    "    div_cumsum = dividends.cumsum()\n",
    "\n",
    "    return div_cumsum.div(div_cumsum.sum(axis=1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_returns(prices):\n",
    "    \"\"\"\n",
    "    Generate returns for ticker and date.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices : DataFrame\n",
    "        Price for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns : Dataframe\n",
    "        The returns for each ticker and date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # As we did before we just divide the daily price by the \n",
    "    # the price on the day before, less one.\n",
    "\n",
    "    return (prices/prices.shift(1) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weighted_returns(returns, weights):\n",
    "    \"\"\"\n",
    "    Generate weighted returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    weights : DataFrame\n",
    "        Weights for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_returns : DataFrame\n",
    "        Weighted returns for each ticker and date\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(weights.index)\n",
    "    assert returns.columns.equals(weights.columns)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # Just return the dataframes multiplied\n",
    "    # every daily return for every ticker by\n",
    "    # every weight of every ticker.\n",
    "\n",
    "    return returns * weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate cumulative returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cumulative_returns : Pandas Series\n",
    "        Cumulative returns for each date\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # First we cumulate the returns per day for all the tickers,\n",
    "    # then we add 1 to use it to multiply each daily return by\n",
    "    # each other.\n",
    "    \n",
    "    return (returns.sum(axis=1)+1).cumprod(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_error(benchmark_returns_by_date, etf_returns_by_date):\n",
    "    \"\"\"\n",
    "    Calculate the tracking error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    benchmark_returns_by_date : Pandas Series\n",
    "        The benchmark returns for each date\n",
    "    etf_returns_by_date : Pandas Series\n",
    "        The ETF returns for each date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tracking_error : float\n",
    "        The tracking error\n",
    "    \"\"\"\n",
    "    assert benchmark_returns_by_date.index.equals(etf_returns_by_date.index)\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # Just implement the function exactly as it is with ddof = 1\n",
    "    # for the sample std\n",
    "\n",
    "    return np.sqrt(252)*(np.std(etf_returns_by_date-benchmark_returns_by_date, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_covariance_returns(returns):\n",
    "    \"\"\"\n",
    "    Calculate covariance matrices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    returns_covariance  : 2 dimensional Ndarray\n",
    "        The covariance of the returns\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    # Implementing the numpy covariance matrix function,\n",
    "    # with only the argument that each series is column base\n",
    "    \n",
    "    return np.cov(returns.fillna(0), rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-46a959b0489e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcvx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_optimal_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moptimal\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "def get_optimal_weights(covariance_returns, index_weights, scale=2.0):\n",
    "    \"\"\"\n",
    "    Find the optimal weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    covariance_returns : 2 dimensional Ndarray\n",
    "        The covariance of the returns\n",
    "    index_weights : Pandas Series\n",
    "        Index weights for all tickers at a period in time\n",
    "    scale : int\n",
    "        The penalty factor for weights the deviate from the index \n",
    "    Returns\n",
    "    -------\n",
    "    x : 1 dimensional Ndarray\n",
    "        The solution for x\n",
    "    \"\"\"\n",
    "    assert len(covariance_returns.shape) == 2\n",
    "    assert len(index_weights.shape) == 1\n",
    "    assert covariance_returns.shape[0] == covariance_returns.shape[1]  == index_weights.shape[0]\n",
    "\n",
    "    #TODO: Implement function\n",
    "    # checking the number of stocks\n",
    "    m = len(index_weights)\n",
    "    \n",
    "    # Creating the X variable vector to be optimize\n",
    "    x = cvx.Variable(m)\n",
    "    \n",
    "    # the portfolio variance, in quadratic form\n",
    "    portfolio_variance = cvx.quad_form(x, covariance_returns)\n",
    "    \n",
    "    # Pythagorean theorem (L2 norm) between portfolio and index weights\n",
    "    distance_to_index = cvx.norm(x-index_weights)\n",
    "    \n",
    "    # Creating our objective to minimize portfolio variance and the distance \n",
    "    # of the portfolio weights from the index weights, and the scale constant\n",
    "    # at the same time.\n",
    "    objective = cvx.Minimize(portfolio_variance + scale * distance_to_index)\n",
    "    \n",
    "    # Defining the list of constraints\n",
    "    constraints = [x >= 0, sum(x) == 1]\n",
    "    \n",
    "    # Now use cvxpy to solve the problem and find the objective\n",
    "    problem = cvx.Problem(objective, constraints)\n",
    "    min_val = problem.solve()\n",
    "    \n",
    "    # Finally return the x series with the values\n",
    "    return x.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_portfolio(returns, index_weights, shift_size, chunk_size):\n",
    "    \"\"\"\n",
    "    Get weights for each rebalancing of the portfolio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    index_weights : DataFrame\n",
    "        Index weight for each ticker and date\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    chunk_size : int\n",
    "        The number of days to look in the past for rebalancing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_rebalance_weights  : list of Ndarrays\n",
    "        The ETF weights for each point they are rebalanced\n",
    "    \"\"\"\n",
    "    assert returns.index.equals(index_weights.index)\n",
    "    assert returns.columns.equals(index_weights.columns)\n",
    "    assert shift_size > 0\n",
    "    assert chunk_size >= 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # We male a loop to generate every list of new X weights.\n",
    "    # Our first day will be the chunk_size to guarantee that we have\n",
    "    # enough data to send. We'll jump every shift_size in the loop.\n",
    "    # We then send the covariance of the chunk in returns, and our index\n",
    "    # will be the day we are working for in the loop.\n",
    "       \n",
    "    return [get_optimal_weights(get_covariance_returns(returns.iloc[a-chunk_size:a,:]), \n",
    "                                  index_weights.iloc[a-1,:]) for a in range(chunk_size, len(returns), shift_size)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio_turnover(all_rebalance_weights, shift_size, rebalance_count, n_trading_days_in_year=252):\n",
    "    \"\"\"\n",
    "    Calculage portfolio turnover.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_rebalance_weights : list of Ndarrays\n",
    "        The ETF weights for each point they are rebalanced\n",
    "    shift_size : int\n",
    "        The number of days between each rebalance\n",
    "    rebalance_count : int\n",
    "        Number of times the portfolio was rebalanced\n",
    "    n_trading_days_in_year: int\n",
    "        Number of trading days in a year\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    portfolio_turnover  : float\n",
    "        The portfolio turnover\n",
    "    \"\"\"\n",
    "    assert shift_size > 0\n",
    "    assert rebalance_count > 0\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # First we convert the Ndarray of weights to a Dataframe\n",
    "    all_weight_dataf = pd.DataFrame(all_rebalance_weights)\n",
    "    \n",
    "    # Later we calculate the absolute difference between weights\n",
    "    # one day and the next day. We add all of them and multiply this total by\n",
    "    # the rebalance events per year. Finally we divide it by the rebalance_count\n",
    "    return  (abs(all_weight_dataf-all_weight_dataf.shift(-1)).sum().sum()) * \\\n",
    "                (n_trading_days_in_year / shift_size) / rebalance_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
